{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogs/an-attempt-to-playing-contra-with-machine-learning/","result":{"data":{"site":{"siteMetadata":{"title":"Twistronics Blog"}},"markdownRemark":{"id":"aebc337c-1889-5ab1-832f-86fe4b5e93bd","excerpt":"This article presents an attempt for playing contra on Nintendo Entertainment System (NES) with neural networks. We have tried different network architectures…","html":"<!-- #### Abstract -->\n<p>This article presents an attempt for playing contra on Nintendo Entertainment System (NES) with neural networks. We have tried different network architectures and different strategies of training, but we failed to beat level 1. We mainly focus on the analysis of our failure and possible solutions.</p>\n<h4>Introduction</h4>\n<p>The Nintendo Entertainment System is one of the most successful commercial video game console. It also has a Chinese counterpart, Subor(小霸王学习机), which is part of the childhood of people born in 80s and 90s. Contra is one of the famous games on both NES and Subor. It is of moderate difficulty, not as hard as Mega Man or Ghosts’n Goblins, which is meant for hardcore gamers, and not as easy as Tetris or Breakout for machine to learn or parameterize the action strategies. Contra is also a game of linear gameplay. Since nonlinear games such as Zelda needs long-term strategy and memory, it is almost impossible for neural networks to be trained for such games. </p>\n<h4>The NES emulator</h4>\n<p>There are many NES emulators created and used by nerds who are in favor of old-school 8-bit games. From those NES emulators, we have chosen two of them for the final project, FCEUX and Bizhawk. The two emulator has a lua script interpreter and some built in functions for users to program them, but both has some defects in dueling with socket or dueling with graphics, so we have used them both.</p>\n<p>The NES is based on an 8-bit processor and a RAM of 2048 bytes. The RAM contains all the data needed for the game to run. Due to the limitation of RAM size, NES games fix the memory at fixed physical memory location, which is to say, it is reasonable to scratch information from memory. NES has a screen of 256240 pixels, which is consisted of squares, or sprites, of 88 pixels. Trying to capture the information directly from the screen will generate a 32*30 tensor so that not too much information is lost, but to keep information such as momentum (or velocity) will result in higher dimension of input.</p>\n<h4>Training Method</h4>\n<p>At first, we decided to follow Google Deepmind’s work, using reinforcement learning to\nparameterize out network so that it can react to the different circumstances in the game. At the\ntime, we try to implement Q-learning algorithm in lua, we come over a critical problem. Atari\ngames, comparing to NES games, are far easier.\nWhen you play an Atari game, you want to get a high score. Not many Atari games have a complicated circumstance and DQN does not perform well on them, such as Ms. Pac-Man.\nUnfortunately, when playing an NES game, people do not care about the score, except the\nhardcore players. Beating a level is already hard enough. In addition, NES needs more operations\nand more strategies. Contra is such a complicated game that needs a more complicated\narchitecture, so training it with Q-learning will be far slower than an Atari game. Thus, we\ndecide to train it with supervised learning at first.</p>\n<h4>Network architecture</h4>\n<p>The best game the DQN network performs on is Video Pinball, which Deepmind team claims to\nperform 2539% better than human players do. As far as I am concerned, mastering Video Pinball\nis of equal difficulty of mastering Flappy Bird. There was somebody already done the work.\n<a href=\"https://github.com/SarvagyaVaish/FlappyBirdRL\">FlappyBirdRL</a>\nAn Atari game has a joystick and a button, and for Pinball, it only needs three operations, up,\ndown and stay.</p>\n<p> For Flappy Bird, two operations are needed, touch and release. Parameterizing\nthe network focuses mainly on parameterizing the timing of these operations. Deepmind team\nclaims to have accomplished human-level control through DQN, as they uses only the graphic as\ninput. The network indeed reaches the human-level, but it is limited by its architecture and\ncannot satisfy our need. To duel with the graphic, two convolution layers are used to do down\nsampling. Much information are lost and many noises are imported. In addition, these\nconvolution layers are a great overhead. In the network that plays flappy bird, very simple\nnetwork is used to accomplish the task, may be a linear layer to transform the input, which\nshould be less than 10 nodes, and a sigmoid layer of one dimension to regulate the output. DQN\nuses two layers of fully connected linear layer with ReLU gates to do the similar task. The main\ndifference between DQN and the flappy bird network is that DQN has two convolution layers to\nscratch information from screen, while the latter scratch information directly from memory. Thus\nthe latter can perform far better than human while DQN only reaches a normal standard.</p>\n<p>Based on the above reasoning, we use the memory as input instead of the screen. For\nperformance reasons, we used some of memories instead of all memories.</p>\n<h4>Performance</h4>\n<p>We have tried different neural network architectures, which performed quite differently. Here is\nthe detailed description of different networks.\nTo measure the performance, we train the network with a sequence of 4550 inputs, which are\n4550 frames in the game play, and count the times of the network gives different output\ncompared to the target input. Network architectures are enclosed with this document.</p>\n<h4>Performance of FFNN</h4>\n<h6>Grouped Connection from memory</h6>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/09e48/ffnn1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABJ0AAASdAHeZh94AAABSElEQVQ4y6VUi26DMAzk/3+UVpTxCnk63HxeaUHVpLWLdIrtOJfDiWlwH9u22TwMA7quQ9/3htvthsvlgrZtMU2TzVy/Xq+2vuE8GiaVUlBrNdKUEkKMiHfQ5xxCQMr5EafPmft2cH9TRSAKOrvK98b22GuE4zgi68k/gYqYBcOakYpobsWXS+iXhCUUjD5j1LXZF8zqT+r7VDTtoJCOj1kDP+pCrugdN5KgYImCqolEkWrY/R0nhW5Z0M/BEjWihGIqeSJVbXbyruBpm3+o30nhoJ+VSWgKBWsSHG//HTR+dZjWaGpY4HbUm81yUvJXGKFI0WcjVuRuTlpk+UjZQ+G6rnbLuVS9vfxSl7cVkowP+9OavSgM3ptCewL/UPfsFDWOnXLEb4qP8d0mj/UyyY49yZoyRts5pzbjcrefcZaJRPwX7J3G+DcG10+K9eO7CQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ffnn1\"\n        title=\"ffnn1\"\n        src=\"/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/fcda8/ffnn1.png\"\n        srcset=\"/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/12f09/ffnn1.png 148w,\n/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/e4a3f/ffnn1.png 295w,\n/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/fcda8/ffnn1.png 590w,\n/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/efc66/ffnn1.png 885w,\n/static/88e9dd1a1cd0ad21794cfdb7dcb718ca/09e48/ffnn1.png 974w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Trained with method that focus on special cases. Here is the pseudo code</p>\n<div class=\"gatsby-highlight\" data-language=\"ruby\"><pre class=\"language-ruby\"><code class=\"language-ruby\"><span class=\"token keyword\">for</span> i <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4550</span> <span class=\"token keyword\">do</span>\n  calculate loss with input <span class=\"token keyword\">and</span> target\n  <span class=\"token keyword\">if</span> loss <span class=\"token operator\">></span> threshold<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">then</span>\n      record input<span class=\"token punctuation\">,</span> target\n      train with all previous records <span class=\"token keyword\">for</span> <span class=\"token number\">2</span> times\n  <span class=\"token keyword\">end</span>\n  train with this <span class=\"token keyword\">case</span>\n<span class=\"token keyword\">end</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4f9a554f461a1d87570db4c3728922c5/58213/ffn2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABJ0AAASdAHeZh94AAABcklEQVQ4y6VT2ZKDMAzj/391S0tucmott2Eo04c9MuOJbWRbUcKSUkJrDfuesG0bnA8wxsI5By8WY4S1Fvd11VwIXmMnRjwthIAxhtqy7ztyzphrhAfQK367eu/PhmTApgyaW9GS073vAb1I/gWu9obOvL+jEufuaOIrTmqPhmRXSkVvFUMaKEvxezRazJ3FQwpLrdh5mt4UU0SmVstx3IOh6igTrE+oJcsHqaGRXbTSzGv8CAWrL6htwMcEFyKqDCG7gyEvhEnhBRMrjCOovATtOijXhk2alTY0vbqM1cjlJKnlJJwY8taCsBzjOcXGgq/NizlsNuBmxCQ2LshJIrLoHYRZ45HbEz9rtaFehrDkfty0GJmFPBm88oKpwrbVekLiXUNlGKIGUwudJkYxxyk3WVyxbxpeGZ6n/cUWvvKzhv+1hTpMhleWP/VnzLVQI75FPh025kPnfvX5N33y+YZnLfPL+T/85M/Fgk8+m008/W+JxE7lsPeErgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ffn2\"\n        title=\"ffn2\"\n        src=\"/static/4f9a554f461a1d87570db4c3728922c5/fcda8/ffn2.png\"\n        srcset=\"/static/4f9a554f461a1d87570db4c3728922c5/12f09/ffn2.png 148w,\n/static/4f9a554f461a1d87570db4c3728922c5/e4a3f/ffn2.png 295w,\n/static/4f9a554f461a1d87570db4c3728922c5/fcda8/ffn2.png 590w,\n/static/4f9a554f461a1d87570db4c3728922c5/efc66/ffn2.png 885w,\n/static/4f9a554f461a1d87570db4c3728922c5/58213/ffn2.png 902w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/18f1e763819e34a961b8515e14af312e/636c2/ffnn3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABJ0AAASdAHeZh94AAABcUlEQVQ4y6VT25aDIAz0/3+yb20Vr4CicslmUum6rm4flnPiISSZDBMsUkqEFWOk+/1OSinq+57KspR907Skqooejwc9n0/2G+q6TmLIqThm7SgYwCpoW3CGYRAzxkiRtZbuDASw2+0mprWmcbScp6Ux8pdl+QZc11XYZabHhYK6rpmJEnbIPVuoF0CNDgyaODGe2HHFi7xMqhiniRY3UXSG4oeCTyaA0GmeHXnTUgzhX4CwwhhNbvE0NBXTY8BE/2OIiQVmhomZjkUPntJLZXzeYsN9ndDuLO7221DwRDD2wB00PwXbKjJtRZ0qyfWKomnI61okCZOhsEwbo3R6mzdgnqgkMmPvmfW4kHHbC/ArBR5cGAdpEGwn/lH3YuIp4y2CbtYhGy64+t3Z7spoEJ2VZqdXvhT6cggXV7b2N2BmK7Z1Poulow+Ggacqv9+mBSYedrqc+X/FCmwyQ3TAYdYU5pz7oTE0zzHUzfP89hH7Ar5nmmi1DJAiAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ffnn3\"\n        title=\"ffnn3\"\n        src=\"/static/18f1e763819e34a961b8515e14af312e/fcda8/ffnn3.png\"\n        srcset=\"/static/18f1e763819e34a961b8515e14af312e/12f09/ffnn3.png 148w,\n/static/18f1e763819e34a961b8515e14af312e/e4a3f/ffnn3.png 295w,\n/static/18f1e763819e34a961b8515e14af312e/fcda8/ffnn3.png 590w,\n/static/18f1e763819e34a961b8515e14af312e/efc66/ffnn3.png 885w,\n/static/18f1e763819e34a961b8515e14af312e/636c2/ffnn3.png 911w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4>Related works</h4>\n<p>There are some other works related to playing NES games by machine, let along the work done\nby Google Deepmind group. These works also inspired our work.</p>\n<h4>Lexicographic ordering approach</h4>\n<p>Dr. Tom Murphy does this work. Here is the homepage for his work.\n<a href=\"http://www.cs.cmu.edu/~tom7/mario/\">mario</a>\nConsidering the performance, his work is currently the most successful approach toward\nautomating computer to play an NES game. However, lexicographic ordering the memory of\nNES game is very memory consuming. For linear games, his approach works very well,\nespecially on Mario and Punch-out. When it comes to non-linear games, such as Zelda, the\nalgorithm fails to figure out what to do, just as what we would expect.</p>\n<h3>Neuroevolution</h3>\n<p>Software engineer SethBling implemented the neuroevolution algorithm on lua to learn the play\nof Super Mario. To be honest, his work outperforms ours a lot. We kind of regret to have\nabandoned the neuroevolution approach.\nThe advantage of neuroevolution is that the network can dynamically grow, adding new neurons\nso that it can fit into new circumstances. In other words, these neurons act as memory cells in the\nnetwork.</p>\n<p>When Deepmind released the paper of DNC, we shifted our focus onto differentiable neural\ncomputer (DNC) and neural Turing machine (NTM). NTM is the ancestor of DNC, while DNC\nhas improved external memory management mechanism. Unfortunately, the NTM implemented\non lua by other people is unusable and we failed to fix that out, so we did not use these neural\nnetworks. As a result, we did not use a neural network with dynamically allocated memory.</p>\n<h3>Further analysis</h3>\n<p>First, I would like to analyze our goal, automate the playing of Contra.</p>\n<p>As far as I am concerned, let the machine learn to play the game by itself, or parameterize the\nneural network by reinforcement learning is far more difficult than let it learn by supervised\nlearning. To train the network by RL, the network should predict or explore a possible suited\ntarget by itself. On the other hand, by supervised learning, human provides the target.</p>\n<p>Q-learning, which is the strategy used by DQN to explore a possible target, is shortsighted\ninevitably on the long term. Lexicographic ordering provides the best target in short term, but the\nmemory size limits the future it can search from, thus it is also shortsighted and may lead to a\ndead end if no further mechanism is provided. For example, the character in the game may\nperform a jump that leads him to death after 20 frames. The penalty for a death may not be\ncounted if lexicographic ordering limits it depth below 20 frames. Also, the decay in Q-learning\npenalty for future time, which is often presented by a hyperparameter gamma, may also leads to\nthe failure of avoiding such pits.</p>\n<p>Playing an NES game is a precise task, many shortsighted behavior leads to failure of the game.\nThis is quite different than playing an Atari game. when you fail, you still get a score.</p>\n<p>Let us put the problem in a mathematical way. We have eight buttons, each is ether pressed or\nnot pressed. We also has the memory of 2048 bytes. By eliminating the useless data, we still\nhave 500 bytes to classify the behavior of the eight buttons, which is to say, pressed or released.\nSince reinforcement learning often leads to pits, we choose supervised learning. The actions I\nprovided for the game, altogether for 4500 frames, along with the data we extract from game\nmemory, consists a policy space:\n[0, 255]^500 —> {0, 1}^8</p>\n<p>What we try to train is to let the neural network classify 4500 given circumstances to given\nactions. To play though the game, only non-critical circumstances are allowed to give an output\nof wrong action. Critical circumstances, such as jumping over pits, avoiding an enemy or bullet,\nare not allowed to fail. The network is also required to have some special behaviors. For\nexample, to fire a bullet, it must press and release B button alternatively, given similar\ncircumstances. This feature is provided by taking previous output as input, but it also makes the\noutput rely greatly on previous output, thus the original input.</p>\n<p>Suppose the number a byte stands for is continuous. Let me explain it this way. Since a byte can\npresent 256 numbers, when applying linear transformation to it, the discontinuity between\nintegers can be ignored. Then we can consider [0, 255] as R, to make problem simpler. Is there\nalways a hyperplane that can split data set of R^255 to two spaces so that 4500 given point in the\nspace can be classified correctly? From the many trials and errors my team has experienced, my\nanswer is, in the circumstance of this game, NO. Linear transformation and mappings such as\ntanh and sigmoid do not change the basic properties of the space. Adding quadratic function may\nbe a good idea, but it costs much more computation and will only provide a slightly better shape\nof the plane to separate actions. To make problem more difficult, the similar conditions may\nneed different actions. For example, jumping over a pit of 20 pixels may be okay, but jumping\nover a pit 21 pixel-wide may be lethal. Continuous condition leads to discontinuous decision. If\nwe simply have this pit width pw as input to judge whether to perform a jump, a transformation\nof sigmoid(pw-20.5) may serve the problem well, as we just need to parameterize the bias.\nHowever, in the case of contra, with the input space of 500 dimensions, this becomes impossible.</p>\n<h4>Possible solutions</h4>\n<p>Since it is impossible to train a simple neural network to play through the game, I raised several\npossible solutions to the problem.</p>\n<ol>\n<li>Less input dimensions.\nInput dimensions is the biggest cause of our failure. Just as DQN fails to beat the AI on\nPinball, high dimensions cursed neural networks to scratch critical information to\nperform well. Convolution layer is an effective approach to down sample the information,\nbut it inevitably losses critical information and introduces noises. In addition, since it is\nhard for human to judge the quality of convolution layer output, no one really knows\nwhat information the network scratches from that graphic. In our case, we choose several\nLinear layer along with Tanh Layer to scratch information from memory. Just as the\nresult shows, we failed. Information flow through an unknown mysterious and strange\nway to subsequent layers. There is an absolute effective method, also the ultimate\nsolution, using HNN, human neural network. In other words, we would spend more time\non Contra to investigate further into its memory structure and game mechanism. That is\nto say, calculate the position of player and distances from enemies and tiles, and then\nsend the data directly toward neural network. This will be an input of less than 50\ndimensions.</li>\n<li>Neural network with memory.\nWe failed to fix the compatibility problem of torch-ntm with torch-nn. NTM is not a\nusual view in machine learning society, so it is not well maintained and well tested.\nDQN, the precedent of NTM is not implemented in lua yet. Implementing or maintain\nsuch a module needs further efforts into torch, which we can do only in the future.\nNeuroevolution, though mainly consists of simple neurons, has the ability to dynamically\nallocate new neuron, thus acquire the ability to hold memory. Other concepts in\nneuroevolution, such as mutate, also provide further insights into training neural network.\nOur next step will be returning to neuroevolution.</li>\n<li>Other approach.\nDr. Tom Murphy’s work has inspired us a lot. Dueling inconsistency in policy space\ngiven similar input space with neural network, especially linear transformation may not\nbe a good idea. Something like decision tree may do a better job.\nOur training method also needs some improvement. Treating a game play through as\nsequential input is not the best approach. I once designed an attention system in training,\nwhich resides in some part of the script. It increase the training loss greatly, but it focus\non special cases. After training for certain cases, when the network meets a case that has\na relatively large loss, it records this case, and then it is trained from the recorded cases\nfor several times and returns to the origin training sequence. On most times, the network\nis trained with different circumstances but similar action policies. With this training\nstrategy, great emphasis is laid on special cases with different action. However, this\nmethod simply fails. We may try it with different architecture in the future.</li>\n</ol>\n<h4>Conclusion</h4>\n<p>Our team tried and failed to train a neural network to play contra. It is not a total failure; at least\nthe network learns a “run to right and shoot” strategy. Under more complicated circumstances,\nsuch as avoiding a bullet or jumping over a pit, our neural network fails. From this project, we\nhave learnt to use some important tools, such as Torch7 and Tensorflow. In addition, we acquire\nsome more concrete knowledge of basic neural network layers. Machine learning is a powerful\ntool for automation, we will pay more endeavor into this subject.</p>","frontmatter":{"title":"An attempt to playing contra with machine learning","date":"December 03, 2016","description":"This article presents an attempt for playing contra on Nintendo Entertainment System (NES) with neural networks. We have tried different network architectures and different strategies of training, but we failed to beat level 1. We mainly focus on the analysis of our failure and possible solutions.","tags":["machine learning","机器学习"]}}},"pageContext":{"slug":"/blogs/an-attempt-to-playing-contra-with-machine-learning/","previous":{"fields":{"slug":"/blogs/cs231n-Practical-Machine-Learning-Project-2/"},"frontmatter":{"title":"cs231n: Practical Machine Learning Project 2"}},"next":null}}}